# -*- coding: utf-8 -*-
"""Rabbit Module Test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AmgKEf4W7eRxsL-tdLOz3CSHblsfPLuM
"""

# Import Packages

## Computer Vision

from IPython.display import Image 
from google.colab.patches import cv2_imshow
import cv2
import numpy as np

## Utilities

import os
import glob
import yaml
import time

## Ploting
import matplotlib.pyplot as plt
import matplotlib.dates as md
from scipy.signal import savgol_filter
import pandas as pd 

## Module Files

import rabbit_research as rr
#import cython_video as cyvideo

"""## **Download Videos**"""

rr.fetch_video_from_gdrive(id = '1n0mUW_PNNL2siOhw2u_GORezTod8Ta8r', name = 'rabbit_video_type1_no1')
rr.fetch_video_from_gdrive(id = '1vZhU2yHF52aeO05s1Imb3DUYP696n-yC', name = 'rabbit_video_type1_no2')
rr.fetch_video_from_gdrive(id = '1YTRqYfPxi3aEDZ7njoGbddafarOv5iC5', name = 'rabbit_video_type1_no3')
rr.fetch_video_from_gdrive(id = '1QjbdVAm2_dEGudOC9DZQZ5jqNKapv-MD', name = 'rabbit_video_type1_no4')
rr.fetch_video_from_gdrive(id = '1ck36F4Ut5qAugo7q5LqQlh6WFETx5-7Q', name = 'rabbit_video_type1_no5')
rr.fetch_video_from_gdrive(id = '1NZ_70h3P8let_GmnAYnJKs9Dk5l8q2u9', name = 'rabbit_video_type2_no1')

"""## **Cage Type 1**

### **Get Mask**
"""

## Cage Type 1 - Video No 3

video_multiple_np_eff = rr.read_multiple_videos_efficient(vid_array = ['rabbit_video_type1_no3'], use_gpu = False, sample_frame_rate = 100) 
video_multiple_np_eff = video_multiple_np_eff.reshape(480, 720, 256)

frame_local_maxima, frame_std = rr.get_kde_efficient(arr = video_multiple_np_eff, bandwidth = 'silverman', gridsize = 15, random_sample_size = 200)
mask = rr.get_mask_cage_type1(mask_type = 'combined', std_arr = frame_td, kde_arr = frame_local_maxima, tier1_threshold = 15, lower_threshold = 8)
rr.numpy_io(operation = 'write', filename = 'mask_cage_type1_no3.npy', arr = mask)

backgroumd_img = rr.get_background_image(arr = video_multiple_np_eff, mask = None)
rr.numpy_io(operation = 'write', filename = 'background_cage_type1_no3.npy', arr = backgroumd_img)

cv2.imwrite('background_cage_type1_no3.png', backgroumd_img)

"""### **Heat Map**"""

heatmap_cage_type1_no3 = rr.get_heat_map_cage_type1(vid = 'rabbit_video_type1_no3', mask = mask)

rgba = cv2.cvtColor(np.full((350, 530, 3), (255,255,0), np.dtype('uint8')), cv2.COLOR_RGB2RGBA)
rgba[:,:,3] = rr.normalize_heat_map(heatmap_cage_type1_no3)*255

cv2_imshow(rr.normalize_heat_map(heatmap_cage_type1_no3)*255)
cv2_imshow(rgba)
cv2_imshow(rr.blend_heatmap('background_cage_type1_no3.png', heatmap_cage_type1_no3, threshold = 0.99))

"""## **Cage Type 2**

### **Get Mask**
"""

np_eff_cage_type2 = rr.read_videos_efficient_cage_type2(vid_array = ['rabbit_video_type2_no1'], use_gpu = True, 
                                                        sample_frame_rate = None, stop_frame = None, crop_parameters = [195, 440, 160, 565])

pixel_std_cage_type2 = rr.pixel_std(arr = np_eff_cage_type2)
peak_std_cage_type2 = rr.get_peak_sharpness(arr = np_eff_cage_type2, bandwidth= 'silverman', adj_constant= 30, random_sample_size = 2000)
mask_cage_type2 = rr.get_mask_cage_type2(arr = np_eff_cage_type2, frame_std = pixel_std_cage_type2, peak = peak_std_cage_type2)
rr.numpy_io(operation = 'write', filename = 'mask_cage_type2_no1.npy', arr = mask_cage_type2)

backgroumd_img_type2 = rr.get_background_image(arr = np_eff_cage_type2, mask = None)
rr.numpy_io(operation = 'write', filename = 'background_cage_type2_no1.npy', arr = backgroumd_img_type2)
cv2.imwrite('background_cage_type2_no1.png', backgroumd_img_type2)

"""### **Heat Map**"""

heatmap_cage_type2_no1 = rr.get_heat_map_cage_type2(vid = 'rabbit_video_type2_no1', mask = mask_cage_type2, crop_parameters = [195, 440, 160, 565])

rgba = cv2.cvtColor(np.full((heatmap_cage_type2_no1.shape[0], heatmap_cage_type2_no1.shape[1], 3), (255,255,0), np.dtype('uint8')), cv2.COLOR_RGB2RGBA)
rgba[:,:,3] = rr.normalize_heat_map(heatmap_cage_type2_no1)*255

cv2_imshow(rr.normalize_heat_map(heatmap_cage_type2_no1)*255)
cv2_imshow(rgba)
cv2_imshow(rr.blend_heatmap('background_cage_type2_no1.png', heatmap_cage_type2_no1, threshold = 0.99))

"""## **Development Area**"""

# Hourly feeding station map + graph
# Optimize video read speed + kde (only for background, maybe not necessary ???)
# Think about rabbit over the cage problem

"""### **Cython - Read Videos in Efficient way (Background + Mask)**"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext Cython

# Commented out IPython magic to ensure Python compatibility.
# %%cython
# cimport cython
# cimport numpy as np
# import numpy as np
# 
# @cython.boundscheck(False)
# @cython.wraparound(False)
# def add_frame_to_efficient_arr(np.npy_uint32[:, :] master_arr, np.npy_uint8[:] frame_flat, int size):
#   for index in range(size):
#     master_arr[index, frame_flat[index]] += 1
#   return master_arr

import time
arr1 = np.zeros((99225, 256), dtype='uint32')
arr2 = cv2.imread('experiment_2_cage.png', cv2.IMREAD_GRAYSCALE).flatten()

start_time = time.time()
for i in range(100):
  arr1[np.arange(int(arr1.shape[0])), arr2] += 1
print("--- %s seconds ---" % (time.time() - start_time))

start_time = time.time()
mem = memoryview(arr1)
for i in range(100):
  mem = memoryview(add_frame_to_efficient_arr(mem, arr2, 99225))
print("--- %s seconds ---" % (time.time() - start_time))

def read_videos_efficient_cage_type2(vid_array, crop_parameters, sample_frame_rate = None):
  
  ### Initial declarations
  start_time = time.time()
  cap = cv2.VideoCapture(vid_array[0])     

  ### Create empty Numpy Array 2D with UINT32 type
  if crop_parameters is None:
    video_np = np.zeros((int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) * int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), 256), dtype='uint32')
  else:
    c1, c2, c3, c4 = crop_parameters[0], crop_parameters[1], crop_parameters[2], crop_parameters[3]
    video_np = np.zeros(((c2 - c1) * (c4 - c3), 256), dtype='uint32')

  ### Create memoryview
  mem = memoryview(video_np)
  mem_size = int(mem.shape[0])

  ### Loop videos
  for vid in vid_array:
    cap = cv2.VideoCapture(vid)

    ### Random frame list to read
    if sample_frame_rate is not None:
      random_frames = np.random.randint(1, sample_frame_rate+1, size=(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))))
    else:
      random_frames = np.ones((int(cap.get(cv2.CAP_PROP_FRAME_COUNT))), )

    ### Create counter
    counter = 0

    ### Loop the frames and save them in array
    success = True
    while success:
      success, image = cap.read()
      if success and random_frames[counter] == 1:
        if crop_parameters is None:
          mem = add_frame_to_efficient_arr(mem, cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).flatten(), mem_size)
        else:
          mem = add_frame_to_efficient_arr(mem, cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)[c1:c2, c3:c4].flatten(), mem_size)
      counter += 1
      if counter % 10000 == 0:
        print('Video:', vid, '- Frame:', counter, '- Time:', round((time.time() - start_time)))

    # Return statements
  print("Total execution time for extraction of frames (RAM efficient - Cython):", round((time.time() - start_time)), "seconds")
  if crop_parameters is None:
    return np.array(mem)
  else:
    return np.array(mem).reshape(c2 - c1, c4 - c3, 256)

arr_cython = read_videos_efficient_cage_type2_DEV(vid_array = ['rabbit_video_type2_no1'], sample_frame_rate = 5, crop_parameters = [195, 440, 160, 565])

pixel_std_cage_type2 = rr.pixel_std(arr = arr_cython)
peak_std_cage_type2 = rr.get_peak_sharpness(arr = arr_cython, bandwidth= 'silverman', adj_constant= 30, random_sample_size = 2000)
mask_cage_type2 = rr.get_mask_cage_type2(arr = arr_cython, frame_std = pixel_std_cage_type2, peak = peak_std_cage_type2)
cv2_imshow(mask_cage_type2)

"""### **Feeding station hourly graph**


"""

def get_heat_map_interval_cage_type2(vid, mask, crop_parameters, start_frame, stop_frame, feeding_station_row):
  start_time = time.time()
  cap = cv2.VideoCapture(vid)
  cap.set(1, start_frame)

  ## Create a 2D array of type uint32
  frame_heat_map = np.zeros(((crop_parameters[1] - crop_parameters[0]), (crop_parameters[3] - crop_parameters[2])), np.dtype('uint32'))
  frame_heat_map_ratio = np.zeros((stop_frame-start_frame,), np.dtype('float64')) 

  ## Feeding station rectangles
  feed_station = np.zeros(((frame_heat_map.shape[0] - feeding_station_row), (crop_parameters[3] - crop_parameters[2])), np.dtype('uint32'))

  ## Reading video frame by frame
  counter, counter_internal = 0, 0
  success = True
  while success:
    success, image = cap.read()
    if success:

      ### convert each image to grayscale
      gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)[crop_parameters[0]:crop_parameters[1],
                 crop_parameters[2]:crop_parameters[3]]
      thresh, bw_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

      ### Multiply with the mask values (make sure the type is uint8)
      masked_image = bw_img * (mask / 255).astype('uint8')

      ### Adjust the new created array
      mask_adjusted = (masked_image / 255).astype('uint32')
      frame_heat_map += mask_adjusted
      feed_station += mask_adjusted[feeding_station_row:,:]
      frame_heat_map_ratio[counter_internal] = (mask_adjusted[feeding_station_row:,:].sum() / mask_adjusted.sum())
      counter_internal += 1

    counter += 1
    if counter == stop_frame-start_frame:
      success = False

  ##return to the matrix
  print("Total execution time for heat map:", (time.time() - start_time), "seconds")
  return frame_heat_map_ratio.mean()

frame_heat_map, feed_station, frame_heat_map_ratio = get_heat_map_interval_cage_type2(vid = 'rabbit_video_type2_no1', mask = mask_cage_type2, crop_parameters = [195, 440, 160, 565], 
                                                                start_frame = 0 , stop_frame = 500, feeding_station_row = 210)

rgba = cv2.cvtColor(np.full((feed_station.shape[0], feed_station.shape[1], 3), (0,0,255), np.dtype('uint8')), cv2.COLOR_RGB2RGBA)
rgba[:,:,3] = rr.normalize_heat_map(feed_station)*255

cv2_imshow(rr.normalize_heat_map(feed_station)*255)
cv2_imshow(rgba)

feeding_list = []
for minute in range(60):
  feeding_list.append(get_heat_map_interval_cage_type2(vid = 'rabbit_video_type2_no1', 
                                                        mask = mask_cage_type2, 
                                                        crop_parameters = [195, 440, 160, 565], 
                                                        start_frame = minute*1500 , 
                                                        stop_frame = (minute+1)*1500, 
                                                        feeding_station_row = 210))

import matplotlib.pyplot as plt

time = np.arange(0, 60, 1)
feeding_list_np = np.array(feeding_list)

fig, ax = plt.subplots()
ax.plot(time, feeding_list_np)
ax.set(xlabel='time (minutes)', ylabel='Feeding Ratio',
       title='Feeding Station Occupancy Chart')
ax.grid()
fig.savefig("feeding_station_per_minute_test.png")
plt.show()

"""### **Rabbit Over Platform**

"""

mask_cage_type2 = rr.numpy_io(operation = 'read', filename = 'mask_experiement_2.npy', arr = None)

# Developement of rabbit over the layer detection

## Parameters

crop_parameters = [195, 440, 150, 575]
platform_parameters = [97, 180]
minute, second = 15, 3

## Read Video
cap = cv2.VideoCapture('rabbit_video_type2_no1')
cap.set(1, (minute*60+second)*25)
success, image = cap.read()

## Get binary image
gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
thresh, bw_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
bw_img = bw_img[platform_parameters[0]:platform_parameters[1],:]

## Calculate contours
contours = cv2.findContours(bw_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
contours = contours[0] if len(contours) == 2 else contours[1]

## Threshold contour area
result = bw_img.copy()
for c in contours:
  area = cv2.contourArea(c)
  if area < 500:
    cv2.drawContours(result, [c], -1, color=(0, 0, 0), thickness=cv2.FILLED)

## Use openning structural element
kernel = np.ones((20,20),np.uint8)
kernel2 = np.ones((1,25),np.uint8)
opening = cv2.morphologyEx(result, cv2.MORPH_OPEN, kernel)
opening = cv2.morphologyEx(opening, cv2.MORPH_OPEN, kernel2)

## Return sample images
print('Black and white image:\n')
cv2_imshow(bw_img)
print('\n')
print('Result image before morphology operation:\n')
cv2_imshow(result)
print('\n')
print('Result image after morphology operation:\n')
cv2_imshow(opening)

# Estimate upper platform usage

def estimate_platform_usage(vid, mask, crop_parameters, platform_parameters):
  start_time = time.time()
  cap = cv2.VideoCapture(vid) 
  platform_use_frame = np.zeros((int(cap.get(cv2.CAP_PROP_FRAME_COUNT)/25)), np.dtype('uint16'))

  ## Reading video frame by frame
  counter, counter_internal = 0, 0
  success = True
  while success:
    success, image = cap.read()
    if success and counter % 25 == 0:

      ### convert each image to grayscale
      gray_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
      thresh, bw_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
      bw_img = bw_img[platform_parameters[0]:platform_parameters[1],:]

      ### Find contours
      contours = cv2.findContours(bw_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
      contours = contours[0] if len(contours) == 2 else contours[1]

      ### Fill contours less than 1000 pixels
      result = bw_img.copy()
      for c in contours:
        area = cv2.contourArea(c)
        if area < 500:
          cv2.drawContours(result, [c], -1, color=(0, 0, 0), thickness=cv2.FILLED)
      
      ### Use 20x20 kernel for openning operaiton
      kernel = np.ones((20,20),np.uint8)
      kernel2 = np.ones((1,25),np.uint8)
      opening = cv2.morphologyEx(result, cv2.MORPH_OPEN, kernel)
      opening = cv2.morphologyEx(opening, cv2.MORPH_OPEN, kernel2)

      ### Adjust the new created array
      platform_use_frame[counter_internal] = np.count_nonzero(opening)
      counter_internal += 1 

    counter += 1
    if counter % 5000 == 0:
      print('Computation stage:', counter)

  ## Return statements
  print("Total execution time for platform usage:", (time.time() - start_time), "seconds")
  return platform_use_frame
    
platform_usage_cage_type2 = estimate_platform_usage('rabbit_video_type2_no1', mask_cage_type2, crop_parameters = [195, 440, 150, 575], platform_parameters = [97, 180])

platform_usage_cage_type2 = estimate_platform_usage('rabbit_video_type2_no1', mask_cage_type2, crop_parameters = [195, 440, 150, 575], platform_parameters = [97, 180])

# Plot Upper Platform Usage

def plot_upper_platform_usage(arr, apply_savgol_filter = False, filename = None):
  
  ## Time component
  time_arr = np.arange(0, arr.shape[0], 1)
  x_axis = pd.to_datetime(time_arr/60, unit='m')
  xfmt = md.DateFormatter('%H:%M:%S')

  ## Y component and smoothing
  if apply_savgol_filter:
    y_axis = savgol_filter(arr, 51, 3)
  else:
    y_axis = arr

  ## Plot 
  fig, ax = plt.subplots(figsize = (20, 10))
  ax.plot(x_axis, y_axis, color = 'green')
  ax.set(xlabel='Time', ylabel='Detected White Pixel Count (After Morphology)', title='Upper Platform Usage')
  ax.grid()
  
  ### X-axis ticks
  ax.xaxis.set_major_formatter(xfmt)
  plt.xticks(rotation=90)
  minor_ticks = np.arange(0, arr.shape[0]+1, 60)
  x_axis_minor = pd.to_datetime(minor_ticks/60, unit='m')
  ax.set_xticks(x_axis_minor)

  plt.show()
  if filename is not None:
    fig.savefig(filename)

  ## Return void

plot_upper_platform_usage(platform_usage_cage_type2, apply_savgol_filter = False, filename = 'upper_platform_usage_not_smoothed.png')
plot_upper_platform_usage(platform_usage_cage_type2, apply_savgol_filter = True, filename = 'upper_platform_usage_smoothed.png')

"""### **Optical Flow**

#### **Sparse Optical Flow**
"""

mask_cage_type2 = rr.numpy_io(operation = 'read', filename = 'mask_experiement_2.npy', arr = None)

crop_parameters = [195, 440, 160, 565]
mask = mask_cage_type2
cap = cv2.VideoCapture('rabbit_video_type2_no1')
cap.set(1, 5)



success, frame = cap.read()
### convert each image to grayscale
gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
thresh, bw_img = cv2.threshold(gray_img, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)      

### Multiply with the mask values (make sure the type is uint8)
masked_image = bw_img * (mask/255).astype('uint8')

contours = cv2.findContours(masked_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
contours = contours[0] if len(contours) == 2 else contours[1]

### Fill contours less than 1000 pixels
result = masked_image.copy()
for c in contours:
  area = cv2.contourArea(c)
  if area < 0.5:
    cv2.drawContours(result, [c], -1, color=(0, 0, 0), thickness=cv2.FILLED)

cv2_imshow(result)
cv2_imshow(masked_image)

# params for ShiTomasi corner detection
feature_params = dict( maxCorners = 1000,
                       qualityLevel = 0.3,
                       minDistance = 2,
                       blockSize = 2 )
p0 = cv2.goodFeaturesToTrack(result, mask = None, **feature_params)

BGR_img = cv2.cvtColor(result, cv2.COLOR_GRAY2BGR)
for element in range(p0.shape[0]):
  cv2.circle(BGR_img, (int(p0[element, 0, 1]),int(p0[element, 0, 0])), 3 , [0, 0, 255],-1)

cv2_imshow(BGR_img)

crop_parameters = [195, 440, 150, 575]
cap = cv2.VideoCapture('rabbit_video_type2_no1')

# params for ShiTomasi corner detection
feature_params = dict( maxCorners = 1000,
                       qualityLevel = 0.3,
                       minDistance = 7,
                       blockSize = 7 )

# Parameters for lucas kanade optical flow
lk_params = dict( winSize  = (30,30),
                  maxLevel = 2,
                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

# Take first frame and find corners in it
ret, old_frame = cap.read()
old_frame = old_frame[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)

# Create a mask image for drawing purposes
total_distance_list = []

counter = 0
temp_distance = 0

success = True
while success:
  success, frame = cap.read()
  if counter % 5 == 0:
    frame = frame[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # calculate optical flow
    p1, st, err = cv2.optflow.calcOpticalFlowDenseRLOF(old_gray, frame_gray, p0, None, **lk_params)

    # Select good points
    good_new = p1[st==1]
    good_old = p0[st==1]

    # draw the tracks
    total_distance = 0
    for i,(new,old) in enumerate(zip(good_new, good_old)):
        total_distance += np.linalg.norm(new-old)
    temp_distance += total_distance
    
    if counter % 25 == 0:
      total_distance_list.append(temp_distance)
      temp_distance = 0
    
    # Now update the previous frame and previous points
    old_gray = frame_gray.copy()
    p0 = good_new.reshape(-1,1,2)

  counter += 1

cap.release()

plot_upper_platform_usage(np.array(total_distance_list), apply_savgol_filter = True, filename = 'upper_platform_usage_not_smoothed.png')

"""#### **Dense Optical Flow**"""

# Dense optical flow

crop_parameters = [195, 440, 160, 565]
cap = cv2.VideoCapture('rabbit_video_type2_no1')
cap.set(1, 0)

# Take first frame and find corners in it
ret, old_frame = cap.read()
old_frame = old_frame[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)
hsv = np.zeros_like(old_frame)
hsv[...,1] = 255

thresh, bw_img = cv2.threshold(old_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)      
masked_image = bw_img * (mask/255).astype('uint8')
contours = cv2.findContours(masked_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
contours = contours[0] if len(contours) == 2 else contours[1]
old_gray = masked_image.copy()
for c in contours:
  area = cv2.contourArea(c)
  if area < 0.5:
    cv2.drawContours(old_gray, [c], -1, color=(0, 0, 0), thickness=cv2.FILLED)

total_distance_list = []

counter = 0
temp_total_mag = 0

success = True
while success:
  success, frame = cap.read()
  if counter % 5 == 0:
    frame = frame[crop_parameters[0]:crop_parameters[1], crop_parameters[2]:crop_parameters[3]]
    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    thresh, bw_img = cv2.threshold(frame_gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)      
    masked_image = bw_img * (mask/255).astype('uint8')
    contours = cv2.findContours(masked_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    contours = contours[0] if len(contours) == 2 else contours[1]
    frame_gray = masked_image.copy()
    for c in contours:
      area = cv2.contourArea(c)
      if area < 0.5:
        cv2.drawContours(frame_gray, [c], -1, color=(0, 0, 0), thickness=cv2.FILLED)

    flow = cv2.calcOpticalFlowFarneback(old_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)
    mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])
    hsv[...,0] = ang*180/np.pi/2
    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)
    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)

    temp_total_mag += np.mean(mag)      
    if counter % 25 == 0:
      total_distance_list.append(temp_total_mag)
      temp_total_mag = 0

    # Now update the previous frame 
    old_gray = frame_gray.copy()

  counter += 1

cap.release()

plot_upper_platform_usage(np.array(total_distance_list)/5, apply_savgol_filter = True, filename = 'upper_platform_usage_not_smoothed.png')

mag.flatten()

from scipy.stats import skew
from scipy.stats import kurtosis

skew(mag.flatten())
kurtosis(mag.flatten())

np.std(mag)